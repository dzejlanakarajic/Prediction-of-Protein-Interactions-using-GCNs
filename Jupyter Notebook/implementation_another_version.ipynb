{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "for-testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFt2av7bAorx",
        "outputId": "e550709a-6999-4b89-a16e-eef7a063afa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget 'http://snap.stanford.edu/deepnetbio-ismb/ipynb/yeast.edgelist'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-27 14:26:13--  http://snap.stanford.edu/deepnetbio-ismb/ipynb/yeast.edgelist\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11758544 (11M) [text/plain]\n",
            "Saving to: ‘yeast.edgelist’\n",
            "\n",
            "yeast.edgelist      100%[===================>]  11.21M  2.86MB/s    in 3.9s    \n",
            "\n",
            "2020-09-27 14:26:18 (2.86 MB/s) - ‘yeast.edgelist’ saved [11758544/11758544]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1KDsY8v0z1V"
      },
      "source": [
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "import torch.nn.modules.loss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "import torch.nn.modules.loss\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from torch import optim\n",
        "from torch.nn.modules.module import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mirFVQuQvN6"
      },
      "source": [
        "def load_data():\n",
        "  adj = nx.adjacency_matrix(nx.read_edgelist(\"yeast.edgelist\",\n",
        "                                                   delimiter='\\t',\n",
        "                                                   create_using=nx.Graph()))\n",
        "  features = sp.identity(adj.shape[0])\n",
        "  features = torch.FloatTensor(np.array(features.todense()))\n",
        "\n",
        "  return adj, features\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape\n",
        "\n",
        "\n",
        "def mask_test_edges(adj):\n",
        "    # Function to build test set with 2% positive links\n",
        "    # Remove diagonal elements\n",
        "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "    adj.eliminate_zeros()\n",
        "\n",
        "    adj_triu = sp.triu(adj)\n",
        "    adj_tuple = sparse_to_tuple(adj_triu)\n",
        "    edges = adj_tuple[0]\n",
        "    edges_all = sparse_to_tuple(adj)[0]\n",
        "    num_test = int(np.floor(edges.shape[0] / 50.))\n",
        "    num_val = int(np.floor(edges.shape[0] / 50.))\n",
        "\n",
        "    all_edge_idx = list(range(edges.shape[0]))\n",
        "    np.random.shuffle(all_edge_idx)\n",
        "    val_edge_idx = all_edge_idx[:num_val]\n",
        "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "    test_edges = edges[test_edge_idx]\n",
        "    val_edges = edges[val_edge_idx]\n",
        "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "    def ismember(a, b):\n",
        "        rows_close = np.all((a - b[:, None]) == 0, axis=-1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    test_edges_false = []\n",
        "    while len(test_edges_false) < len(test_edges):\n",
        "        n_rnd = len(test_edges) - len(test_edges_false)\n",
        "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
        "        idxs_i = rnd[:n_rnd]                                        \n",
        "        idxs_j = rnd[n_rnd:]\n",
        "        for i in range(n_rnd):\n",
        "            idx_i = idxs_i[i]\n",
        "            idx_j = idxs_j[i]\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if test_edges_false:\n",
        "                if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
        "                    continue\n",
        "                if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
        "                    continue\n",
        "            test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "    val_edges_false = []\n",
        "    while len(val_edges_false) < len(val_edges):\n",
        "        n_rnd = len(val_edges) - len(val_edges_false)\n",
        "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
        "        idxs_i = rnd[:n_rnd]                                        \n",
        "        idxs_j = rnd[n_rnd:]\n",
        "        for i in range(n_rnd):\n",
        "            idx_i = idxs_i[i]\n",
        "            idx_j = idxs_j[i]\n",
        "            if idx_i == idx_j:\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], train_edges):\n",
        "                continue\n",
        "            if ismember([idx_j, idx_i], train_edges):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], val_edges):\n",
        "                continue\n",
        "            if ismember([idx_j, idx_i], val_edges):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
        "                    continue\n",
        "                if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "    # Re-build adj matrix\n",
        "    data = np.ones(train_edges.shape[0])\n",
        "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
        "    adj_train = adj_train + adj_train.T\n",
        "\n",
        "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
        "\n",
        "\n",
        "def preprocess_graph(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    adj_ = adj + sp.eye(adj.shape[0])\n",
        "    rowsum = np.array(adj_.sum(1))\n",
        "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "    # return sparse_to_tuple(adj_normalized)\n",
        "    return sparse_mx_to_torch_sparse_tensor(adj_normalized)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def get_roc_score(emb, adj_orig, edges_pos, edges_neg):\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    adj_rec = np.dot(emb, emb.T)\n",
        "    preds = []\n",
        "    pos = []\n",
        "    for e in edges_pos:\n",
        "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "        pos.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "    preds_neg = []\n",
        "    neg = []\n",
        "    for e in edges_neg:\n",
        "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "        neg.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    roc_score = roc_auc_score(labels_all, preds_all)\n",
        "    ap_score = average_precision_score(labels_all, preds_all)\n",
        "\n",
        "    return roc_score, ap_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJmTFsDyRSA1"
      },
      "source": [
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout=0., act=F.relu):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        input = F.dropout(input, self.dropout, self.training)\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        output = self.act(output)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSYEX1YTTzwR"
      },
      "source": [
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, input_feat_dim, hidden_dim1, hidden_dim2, dropout):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.gc1 = GraphConvolution(input_feat_dim, hidden_dim1, dropout, act=F.relu)\n",
        "        self.gc2 = GraphConvolution(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
        "        self.gc3 = GraphConvolution(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
        "        self.dc = InnerProductDecoder(dropout, act=lambda x: x)\n",
        "\n",
        "    def encode(self, x, adj):\n",
        "        hidden1 = self.gc1(x, adj)\n",
        "        return self.gc2(hidden1, adj), self.gc3(hidden1, adj)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        mu, logvar = self.encode(x, adj)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.dc(z), mu, logvar\n",
        "\n",
        "\n",
        "class InnerProductDecoder(nn.Module):\n",
        "    \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, dropout, act=torch.sigmoid):\n",
        "        super(InnerProductDecoder, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = F.dropout(z, self.dropout, training=self.training)\n",
        "        adj = self.act(torch.mm(z, z.t()))\n",
        "        return adj"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaaDqNgwT0ao"
      },
      "source": [
        "def loss_function(preds, labels, mu, logvar, n_nodes, norm, pos_weight):\n",
        "    cost = norm * F.binary_cross_entropy_with_logits(preds, labels, pos_weight=pos_weight)\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 / n_nodes * torch.mean(torch.sum(\n",
        "        1 + 2 * logvar - mu.pow(2) - logvar.exp().pow(2), 1))\n",
        "    return cost"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyvb1PFAUzbY"
      },
      "source": [
        "import sys\n",
        "sys.argv=['']\n",
        "del sys"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A91pbf8dUERX"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model', type=str, default='gcn_vae', help=\"models used\")\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=20, help='Number of epochs to train.')\n",
        "parser.add_argument('--hidden1', type=int, default=32, help='Number of units in hidden layer 1.')\n",
        "parser.add_argument('--hidden2', type=int, default=16, help='Number of units in hidden layer 2.')\n",
        "parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')\n",
        "parser.add_argument('--dropout', type=float, default=0., help='Dropout rate (1 - keep probability).')\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LIuV8z6VhnY"
      },
      "source": [
        "adj, features = load_data()\n",
        "n_nodes, feat_dim = features.shape\n",
        "\n",
        "# Store original adjacency matrix (without diagonal entries) for later\n",
        "adj_orig = adj\n",
        "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
        "adj_orig.eliminate_zeros()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y32YLgSpVmR5"
      },
      "source": [
        "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
        "adj = adj_train"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvdz3uWEVrkR"
      },
      "source": [
        "# Some preprocessing\n",
        "adj_norm = preprocess_graph(adj)\n",
        "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
        "# adj_label = sparse_to_tuple(adj_label)\n",
        "adj_label = torch.FloatTensor(adj_label.toarray())\n",
        "\n",
        "pos_weight = torch.Tensor([float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()])\n",
        "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3YCXflVVybx"
      },
      "source": [
        "model = GCNModel(feat_dim, args.hidden1, args.hidden2, args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPGQ-H-65XYe"
      },
      "source": [
        "hidden_emb = None"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0JIXhOB9c0D",
        "outputId": "81190f91-5c7d-4fbb-801b-849917afb1ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "for epoch in range(args.epochs):\n",
        "        model = GCNModel(feat_dim, args.hidden1, args.hidden2, args.dropout)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "        hidden_emb = None\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        recovered, mu, logvar = model(features, adj_norm)\n",
        "        loss = loss_function(preds=recovered, labels=adj_label,\n",
        "                             mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
        "                             norm=norm, pos_weight=pos_weight)\n",
        "        loss.backward()\n",
        "        cur_loss = loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        hidden_emb = mu.data.numpy()\n",
        "        roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
        "        \n",
        "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
        "              \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
        "              \"time=\", \"{:.5f}\".format(time.time() - t)\n",
        "              )\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
        "print('Test ROC score: ' + str(roc_score))\n",
        "print('Test AP score: ' + str(ap_score))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 train_loss= 1.72634 val_ap= 0.66662 time= 2.48630\n",
            "Epoch: 0002 train_loss= 1.71785 val_ap= 0.65504 time= 2.45810\n",
            "Epoch: 0003 train_loss= 1.72128 val_ap= 0.66917 time= 2.51036\n",
            "Epoch: 0004 train_loss= 1.72793 val_ap= 0.65935 time= 2.50283\n",
            "Epoch: 0005 train_loss= 1.71863 val_ap= 0.65760 time= 2.46922\n",
            "Epoch: 0006 train_loss= 1.72072 val_ap= 0.66139 time= 2.48034\n",
            "Epoch: 0007 train_loss= 1.72448 val_ap= 0.65072 time= 2.53208\n",
            "Epoch: 0008 train_loss= 1.73778 val_ap= 0.68435 time= 2.48932\n",
            "Epoch: 0009 train_loss= 1.73728 val_ap= 0.66749 time= 2.47474\n",
            "Epoch: 0010 train_loss= 1.74353 val_ap= 0.66067 time= 2.49363\n",
            "Epoch: 0011 train_loss= 1.73043 val_ap= 0.67257 time= 2.48594\n",
            "Epoch: 0012 train_loss= 1.72320 val_ap= 0.68850 time= 2.46616\n",
            "Epoch: 0013 train_loss= 1.72853 val_ap= 0.67654 time= 2.48774\n",
            "Epoch: 0014 train_loss= 1.72362 val_ap= 0.65671 time= 2.52933\n",
            "Epoch: 0015 train_loss= 1.73936 val_ap= 0.67461 time= 2.46538\n",
            "Epoch: 0016 train_loss= 1.72653 val_ap= 0.67505 time= 2.53843\n",
            "Epoch: 0017 train_loss= 1.72250 val_ap= 0.67684 time= 2.49360\n",
            "Epoch: 0018 train_loss= 1.71340 val_ap= 0.66604 time= 2.47981\n",
            "Epoch: 0019 train_loss= 1.73372 val_ap= 0.68032 time= 2.47981\n",
            "Epoch: 0020 train_loss= 1.72696 val_ap= 0.67802 time= 2.47069\n",
            "Optimization Finished!\n",
            "Test ROC score: 0.7640315538711775\n",
            "Test AP score: 0.6827956538717264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRdm7Q69d0Tz",
        "outputId": "f15d73df-837d-42e1-fa20-3f6574af6695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(\"Loaded\",adj.shape[0],\"nodes\")\n",
        "print(\"Loaded\",adj.sum(),\"edges\")\n",
        "print()\n",
        "print(\"-- Data format --\")\n",
        "print(\"Full graph adjacency shape:    \", adj.shape, \"\\t\",             type(adj), \"number of indices\", len(adj.indices))\n",
        "print(\"Training graph adjacency shape:\", adj_train.shape, \"\\t\",       type(adj_train), \"number of indices\", len(adj_train.indices))\n",
        "print(\"val_edges:                     \", val_edges.shape, \"\\t\",       type(val_edges))\n",
        "print(\"val_edges_false:               \", len(val_edges_false), \"\\t\\t\",type(val_edges_false))\n",
        "print(\"test_edges:                    \", test_edges.shape,\"\\t\",       type(test_edges))\n",
        "print(\"test_edges_false:              \", len(test_edges_false),\"\\t\\t\",type(test_edges_false))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 6526 nodes\n",
            "Loaded 1018554.0 edges\n",
            "\n",
            "-- Data format --\n",
            "Full graph adjacency shape:     (6526, 6526) \t <class 'scipy.sparse.csr.csr_matrix'> number of indices 1018554\n",
            "Training graph adjacency shape: (6526, 6526) \t <class 'scipy.sparse.csr.csr_matrix'> number of indices 1018554\n",
            "val_edges:                      (10609, 2) \t <class 'numpy.ndarray'>\n",
            "val_edges_false:                10609 \t\t <class 'list'>\n",
            "test_edges:                     (10609, 2) \t <class 'numpy.ndarray'>\n",
            "test_edges_false:               10609 \t\t <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCl2OGtobtDm",
        "outputId": "3fdabf76-e341-42c5-d1f6-5f91a7e81df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import itertools\n",
        "\n",
        "hyperparams = {\n",
        "    \"hidden1\": [256, 128, 64, 32],\n",
        "    \"hidden2\": [64, 32, 16, 8],\n",
        "    \"lr\": [0.001, 0.01, 0.1]}\n",
        "\n",
        "keys = hyperparams.keys()\n",
        "values = (hyperparams[key] for key in keys)\n",
        "combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]\n",
        "print(\"Total combinations:\" ,len(combinations), \".\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total combinations: 48 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxuYrR3ofGiS",
        "outputId": "7247b3d5-4fe1-4c70-c90d-746a25193654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "combinations[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hidden1': 256, 'hidden2': 64, 'lr': 0.001}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD5r7Gy6gp1q",
        "outputId": "f6e0692e-3a75-44b6-c6ba-eebfedc525d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for key in hyperparams.keys():\n",
        "  print(key)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden1\n",
            "hidden2\n",
            "lr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSj3jcnIlmUu",
        "outputId": "2661483a-ae21-44d6-9f0e-0148a62c3a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "combinations[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hidden1': 256, 'hidden2': 64, 'lr': 0.01}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnwSE9Iul6ur"
      },
      "source": [
        "for d in combinations:\n",
        "    print(d['hidden1'], d['hidden2'], d['lr'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}